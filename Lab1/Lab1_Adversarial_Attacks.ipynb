{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guzonghua/saavlabs/blob/main/Lab1/Lab1_Adversarial_Attacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** This blog article and source code describes how to train a model for Traffic Sign Recognition through fine-tuning a pretrained MobileNetV3 Large model\n",
        "\n",
        "Traffic Sign Recognition using PyTorch and Deep Learning\n",
        "\n",
        "https://debuggercafe.com/traffic-sign-recognition-using-pytorch-and-deep-learning/\n",
        "\n",
        "**Update 20/11/2023: the blog author provides a pre-trained model model.pth. You may choose to use it in Step 2 instead of training your own model.**\n",
        "\n",
        "If you choose to train your own model instead, please follow the following steps.\n",
        "\n",
        "Optional: To save training time, you may use the MobileNetV3 Small model instead of MobileNetV3 Large:\n",
        "https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_small.html\n",
        "\n",
        "By changing these lines in model.py:  \n",
        "\n",
        "    model = models.mobilenet_v3_small(pretrained=pretrained)\n",
        "    ...\n",
        "    # Change the final classification head.\n",
        "    model.classifier[3] = nn.Linear(in_features=1024, out_features=num_classes)\n",
        "\n",
        "(You may choose to design your own architecture, or start from a pretrained network, e.g., LeNet, AlexNet or ResNet, and perform transfer learning/fine-tuning.)\n",
        "\n",
        "At the last step \"Testing the Model and Visualizing Class Activation Maps (CAM)\", please record the model accuracy (CAM is not important). Please save the trained DNN model to be used in Step 2. The purpose of this assignment is to obtain a DNN with reasonable accuracy, say >90%, not to obtain the highest accuracy, so do not spend a lot of time tuning hyperparameters.\n",
        "\n",
        "Ref: PyTorch SAVING AND LOADING MODELS\n",
        "https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "\n",
        "**Step 2.** Follow the steps in \"Chapter 3 - Adversarial examples, solving the inner maximization\" in the tutorial \"Adversarial Robustness - Theory and Practice\" 2018 to create attack images:\n",
        "\n",
        "https://adversarial-ml-tutorial.org/adversarial_examples/\n",
        "\n",
        "Use your trained CNN, perform the following steps:\n",
        "\n",
        "2.1 Untargeted attack using Fast Gradient Sign Method (FGSM), using the function fgsm().\n",
        "\n",
        "2.2 Untargeted attack using Projected Gradient Descent, using the function pgd_linf() (You may use the Projected Steepest Descent variant to accelerate the process).\n",
        "\n",
        "2.3 Targeted attack using Projected Gradient Descent, using the function pgd_linf_targ(), which aims to maximize logit of the target class y_targ and minimize logit of the true class y. You may choose any target class, but I suggest using \"Speed Limit\".\n",
        "\n",
        "2.4 Targeted attack using Projected Gradient Descent, using the function pgd_linf_targ2(),  which  aims to maximize logit of the target class y_targ and minimize logit of all the other classes y'.\n",
        "For each attack, report the accuracy of the attacked classifier on the testset, and show visualization of a few examples of successful attack (original image and attacked image).\n",
        "\n",
        "**Submission instructions**: You may use either Google CoLab or your local machine. If you use CoLab, please send your CoLab web link. Otherwise, please submit an ipynb or Python file, along with a short report in PDF containing the test accuracy and visualization of examples, as well as any problems you encountered or any insights you want to share. You may write the report with WORD, or choose to embed the report within the ipynb file, execute it to show all results, then and print it as PDF."
      ],
      "metadata": {
        "id": "d6Mjv7Y0QxhR"
      }
    }
  ]
}