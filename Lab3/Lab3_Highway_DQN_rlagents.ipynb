{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guzonghua/saavlabs/blob/main/Lab3/Lab3_Highway_DQN_rlagents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTBW2COTwOg_"
      },
      "source": [
        "Lab3 is based on highway-env https://github.com/eleurent/highway-env. In \n",
        "envs/highway_env.py, the vehicle is rewarded for reaching a high speed, staying on the rightmost lanes and avoiding collisions for in highway driving (highway-env). Your task is to add an additional term to penalize changing lanes. (Ideally we should penalize changing lanes to the left more than changing to the right, but this is a simplified exercise.) Please refer to envs/roundabout_env for reference.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1) Fork the highway-env repository to your own GitHub account, and modify highway_env.py. (if you are not familiar with GitHub, I recommend installing GitHub Desktop https://desktop.github.com.)\n",
        "\n",
        "2) Replace \n",
        "!pip install highway-env \n",
        "with path to your own repository:\n",
        "!pip install git+https://github.com/USERNAME/highway-env\n",
        "\n",
        "3) Train your agent for different values of lane_change_reward, and different environments with varying complexity, starting from the simplest:\n",
        "\n",
        "env.config[\"lane_change_reward\"] = 0\n",
        "\n",
        "env.config[\"vehicles_count\"] = 0\n",
        "\n",
        "env.config[\"lanes_count\"] = 2\n",
        "\n",
        "env.config[\"initial_lane_id\"] = 0\n",
        "\n",
        "In this simple environment, during testing the vehicle should switch to the right lane quickly and stay in the right lane. (It is possible for it to stay in the left lane during testing if your training num_episodes is too small, since the agent may not have had a chance to try the lane change action during training.)\n",
        "Try different values of lane_change_reward (e.g., -0.1, -1, -10, etc. note that it must be negative), and different numbers of vehicles/lanes. Make lane_change_reward very negative until you observe that the vehicle prefers *collision* to *lane change to avoid collision*. Submit your IPython Notebook, and a short report (or text blocks within the Notebook) describing the different configurations you have tried, and your observations for each config, including two configs that differ only in the lane_change_reward, one with no or rare collisions, one with frequent collisions. \n",
        "\n",
        "\n",
        "You may use either Google CoLab or your local machine. If you use CoLab, please send your CoLab web link. Otherwise, please submit your IPython Notebook to Canvas. Please include the recorded videos during the testing in IPython Notebook. Since it is not possible to save multiple trained models without modifying the source code of rl-agents, we can only see one trained model in Colab that you trained last. It is not realistic for us to retrain all the models, so please provide screenshots and descriptions in the report for your observations of each config, esp. \"the two configs that differ only in the lane_change_reward, one with no or rare collisions, one with frequent collisions\".\n",
        "\n",
        "Notes: \n",
        "\n",
        "1) More complex environments demand more training time (more num_episodes), so do not make your environment more complex than necessary. You may try increasing the  vehicles_density and reward_speed_range to increase the chances of collision. You may also find \"Script to Stop Google Colab From Disconnecting\" to be helpful.\n",
        "\n",
        "2) You may want to tune DQN hyperparams in configs/HighwayEnv/agents/DQNAgent/dqn.json, e.g., discount factor gamma, and epsilon greedy exploration params tau, temperature, final_temperature for different num_episodes.\n",
        "\n",
        "3) The error message \"fatal: destination path 'highway-env' already exists and is not an empty directory\" in the first code cell can be safely ignored.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sepDWoBqdRMK"
      },
      "source": [
        "# Training a DQN on `highway-v0` with rlagents\n",
        "## Import requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx8X4s8krNWt"
      },
      "source": [
        "# Environment\n",
        "!pip install highway-env\n",
        "#!pip install git+https://github.com/USERNAME/highway-env\n",
        "import gym\n",
        "import highway_env\n",
        "\n",
        "# Agent\n",
        "!pip install git+https://github.com/eleurent/rl-agents\n",
        "\n",
        "# Visualisation utils\n",
        "import sys\n",
        "%load_ext tensorboard\n",
        "!pip install tensorboardx gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "!git clone https://github.com/eleurent/highway-env.git\n",
        "sys.path.insert(0, '/content/highway-env/scripts/')\n",
        "from utils import show_videos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvOEW00pdHrG"
      },
      "source": [
        "## Training\n",
        "\n",
        "Prepare environment, agent, and evaluation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowKW3ix45ZW"
      },
      "source": [
        "from rl_agents.trainer.evaluation import Evaluation\n",
        "from rl_agents.agents.common.factory import load_agent, load_environment\n",
        "\n",
        "# Get the environment and agent configurations from the rl-agents repository\n",
        "!git clone https://github.com/eleurent/rl-agents.git\n",
        "%cd /content/rl-agents/scripts/\n",
        "env_config = 'configs/HighwayEnv/env.json'\n",
        "agent_config = 'configs/HighwayEnv/agents/DQNAgent/dqn.json'\n",
        "\n",
        "env = load_environment(env_config)\n",
        "import pprint\n",
        "from matplotlib import pyplot as plt\n",
        "env.config[\"lane_change_reward\"] = 0\n",
        "env.config[\"vehicles_count\"] = 0\n",
        "#env.config['reward_speed_range'] = [0, 100]\n",
        "env.config[\"lanes_count\"] = 2\n",
        "env.config[\"initial_lane_id\"] = 0\n",
        "pprint.pprint(env.config)\n",
        "pprint.pprint(agent_config)\n",
        "\n",
        "env.reset()\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))\n",
        "plt.show()\n",
        "\n",
        "agent = load_agent(agent_config, env)\n",
        "evaluation = Evaluation(env, agent, num_episodes=10, display_env=False)\n",
        "print(f\"Ready to train {agent} on {env}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtK9dtfb0JMF"
      },
      "source": [
        "Start training. Run tensorboard locally to visualize training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFVq1gFz42Eg"
      },
      "source": [
        "%tensorboard --logdir \"{evaluation.directory}\"\n",
        "evaluation.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lNvWg42RWiw"
      },
      "source": [
        "Progress can be visualised in the tensorboard cell above, which should update every 30s (or manually). You may need to click the *Fit domain to data* buttons below each graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKfvu5uhzCIU"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Run the learned policy for a few episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY0rpVYUtRpN"
      },
      "source": [
        "#env = load_environment(env_config)\n",
        "env.configure({\"offscreen_rendering\": True})\n",
        "agent = load_agent(agent_config, env)\n",
        "evaluation = Evaluation(env, agent, num_episodes=3, recover=True)\n",
        "evaluation.test()\n",
        "show_videos(evaluation.run_directory)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}